
services:
  cli:
    build:
      context: ./docker/worker
    command: "python cli.py"
    env_file:
      - ./app.env
    volumes:
      - ./worker:/app
      - ./data/backend:/data
    depends_on:
      - sam2
      - openpose
    restart: on-failure

  sam2:
    build:
      context: ./docker/sam2
    command: "uvicorn main:app --reload --proxy-headers --host 0.0.0.0 --root-path /api/"
    env_file:
      - ./app.env
    volumes:
      - ./sam2:/app
    restart: on-failure
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["${MASKANYONE_GPU_ID:-0}"]
              capabilities: [ gpu ]
    # devices: # use in case the new resource reservation with GPU IDs is not working, uses GPU 0 by default
    #   - /dev/nvidia0:/dev/nvidia0
    #   - /dev/nvidiactl:/dev/nvidiactl
    #   - /dev/nvidia-uvm:/dev/nvidia-uvm
    #   - /dev/nvidia-uvm-tools:/dev/nvidia-uvm-tools

  openpose:
    build:
      context: ./docker/openpose
    command: "uvicorn main:app --reload --proxy-headers --host 0.0.0.0 --root-path /api/"
    env_file:
      - ./app.env
    volumes:
      - ./openpose:/app
      - ./docker/openpose/models:/models
    restart: on-failure
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["${MASKANYONE_GPU_ID:-0}"]
              capabilities: [ gpu ]
    # devices: # use in case the new resource reservation with GPU IDs is not working, uses GPU 0 by default
    #   - /dev/nvidia0:/dev/nvidia0
    #   - /dev/nvidiactl:/dev/nvidiactl
    #   - /dev/nvidia-uvm:/dev/nvidia-uvm
    #   - /dev/nvidia-uvm-tools:/dev/nvidia-uvm-tools